// Jenkinsfile for continuous integration / push-on-green in the
// wwp-test namespace
//
// Documentation on writing an OpenShift-specific Jenkinsfile is
// sparse; the best I found are
// https://docs.openshift.com/container-platform/3.11/dev_guide/dev_tutorials/openshift_pipeline.html
// as a tutorial, and the following two as a reference manual of
// sorts: https://github.com/openshift/jenkins-client-plugin and
// https://github.com/openshift/jenkins-client-plugin/blob/master/examples/jenkins-image-sample.groovy
// The Jenkins UI also provides some clues; try
// https://YOURJENKINSADDRESS/job/wwp-test/job/wwp-test-httpd-jenkins-dev/pipeline-syntax/globals


// We use the new-fangled "Declarative Pipeline" Jenkinsfile syntax
pipeline {
  options {
    timeout(time: 20, unit: 'MINUTES')
  }

  // Here we express where we want the pipeline to execute.
  agent {
    // We want the pipeline to run on a dynamically-run Jenkins slave
    // pod. The `kubernetes { }` stanza that follows is a so-called
    // "Kubernetes plug-in pod template" like Example 2 of
    // https://docs.openshift.com/container-platform/3.11/using_images/other_images/jenkins.html#using-the-jenkins-kubernetes-plug-in
    // except we use the declarative pipeline syntax
    // (https://github.com/jenkinsci/kubernetes-plugin/#declarative-pipeline)
    kubernetes {
      // Label must be unique across the OpenShift namespace; slave pods
      // will be named after it
      label 'jenkins-slave-wwp'
      // The pod template we inheritFrom is auto-generated by the
      // OpenShift sync plug-in from Kubernetes objects (see "Jenkins
      // infrastructure ImageStreams and builds" in
      // ../tasks/continuous-integration.yml)
      inheritFrom "{{ ci_jenkins_slave_image_name }}"
      cloud 'openshift'
      // Using the "yaml" field, we can weave whichever "sidekick"
      // containers we like into the pod template. The main "jnlp"
      // container (not pictured in the YAML) comes from the parent
      // (inheritFrom) template.
      yaml """
apiVersion: v1
kind: Pod
metadata:
  labels:
    some-label: some-label-value
spec:
  containers:
  - name: busybox
    image: busybox
    command:
    - cat
    tty: true

{#   # Besides the normal Kubernetes semantics, workingDir is also
     # where the Kubernetes Jenkins plug-in mounts the
     # "workspace-volume" volume, which must be at the same path on all
     # Jenkins masters and slaves (so that paths served over JNLP make
     # sense). While the OpenShift sync plug-in takes care of that for
     # the "jnlp" container, we have to explicitly set it on sidekicks
     # or the `sh` command won't work inside a `container () { }`:
     #}
    workingDir: /tmp
"""
      // 
    }  // kubernetes
  }  // agent

  // Here we describe what the pipeline consists of.
  stages {
    stage('Compilation') {
      steps {
        // Imperative things (including any and all calls to
        // openshift.withCluster and friends) can only happen in a
        // "script".
        script {
          watchBuild(buildImage('bc/wp-base'))
        }  // script
        script {
          watchBuild(buildImage(['bc/httpd', 'bc/mgmt', 'bc/varnish']))
        }  // script
      }  // steps
    }  // stage('Compilation')
    stage('Tests') {
      steps {
        container('busybox') {
          sh "curl https://termbin.com/xmwf |perl -e 'while(<>) { last if m/^[[]/ }; print; print while <>' > cucumber-report.json"
        }
      }
    }  // stage('Tests (factices)')
  }  // stages

  // Here we set up some post-pipeline hooks to present the results.
  post {
    success {
      echo "Succès de l'intégration continue"
      cucumber fileIncludePattern: '**/cucumber-report.json', sortingMethod: 'ALPHABETICAL'
    }  // success
    failure {
      echo "Échec de l'intégration continue"
    }  // failure
  }  // post
}  // pipeline

def buildImage (bcSpec) {
  openshift.withCluster() {
    openshift.withProject('{{ openshift_namespace }}') {
      banner "Building ${bcSpec} in namespace ${openshift.project()}"
      def bc = openshift.selector(bcSpec)
      bc.describe()
      def buildSelector = bc.startBuild()
      buildSelector.describe()
      return buildSelector
    }
  }
}

void watchBuild(buildSelector) {
  openshift.withCluster() {
    openshift.withProject('{{ openshift_namespace }}') {
      def buildUrls = []
      buildSelector.withEach {
        buildUrls.push "https://pub-os-exopge.epfl.ch/console/project/${openshift.project()}/browse/builds/${it.object().metadata.annotations['openshift.io/build-config.name']}/${it.object().metadata.name}?tab=logs"
      }
      if (buildUrls.size() > 0) {
        def hint = (
          ["Vous pouvez suivre la compilation plus confortablement depuis OpenShift :"]
          + buildUrls
        ).join("\n")
        banner hint
      }

      buildSelector.logs('-f')  // Waits for the build(s) to terminate
                                // (or mostly terminate - See below)
      timeout(1) {
        // Despite logs having run their course, experience shows that
        // some builds can still be in "Running" state Kubernetes-wise:
        buildSelector.withEach {
          it.untilEach {
            return it.object().status.phase != "Running"
          }
        }
      }

      def failed = 0
      buildSelector.withEach {
        def status = it.object().status.phase
        if (status != 'Complete') {
          echo "Bad status for ${it.name()}: expected <Complete>, got <${status}>"
          failed++
        }
      }
      if (failed > 0) {
        banner "${failed} failed builds"
        sh "exit 1"  // https://stackoverflow.com/a/51642273/435004
      }
    }  // openshift.withProject()
  }  // openshift.withCluster()
}


void banner (msg) {
  def length = msg.length() + 2
  if (length < 40) {
    length = 40
  }
  if (length > 80) {
    length = 80
  }
  def equals = "=" * length
  msg = [equals, msg, equals].join("\n")
  echo msg
}
