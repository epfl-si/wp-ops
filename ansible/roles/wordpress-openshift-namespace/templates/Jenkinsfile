// Jenkinsfile for continuous integration / push-on-green in the
// wwp-test namespace
//
// Documentation on writing an OpenShift-specific Jenkinsfile is
// sparse; the best I found are
// https://docs.openshift.com/container-platform/3.11/dev_guide/dev_tutorials/openshift_pipeline.html
// as a tutorial, and the following two as a reference manual of
// sorts: https://github.com/openshift/jenkins-client-plugin and
// https://github.com/openshift/jenkins-client-plugin/blob/master/examples/jenkins-image-sample.groovy
// The Jenkins UI also provides some clues; try
// https://YOURJENKINSADDRESS/job/wwp-test/job/wwp-test-httpd-jenkins-dev/pipeline-syntax/globals


// We use the new-fangled "Declarative Pipeline" Jenkinsfile syntax
pipeline {
  options {
    timeout(time: 20, unit: 'MINUTES')
  }

  // Here we say that this is a nightly build
  triggers {
    // See https://jenkins.io/doc/book/pipeline/syntax/#cron-syntax
    // Note: the time zone is GMT (add 1 or 2 hours to get European
    // time, depending on season)
    cron('H H(2-3) * * *')
  }

  // Here we express where we want the pipeline to execute.
  agent {
    // We want the pipeline to run on a dynamically-run Jenkins slave
    // pod. The `kubernetes { }` stanza that follows is a so-called
    // "Kubernetes plug-in pod template" like Example 2 of
    // https://docs.openshift.com/container-platform/3.11/using_images/other_images/jenkins.html#using-the-jenkins-kubernetes-plug-in
    // except we use the declarative pipeline syntax
    // (https://github.com/jenkinsci/kubernetes-plugin/#declarative-pipeline)
    kubernetes {
      // We run on this Jenkins cloud (the one that Camptocamp set up for us):
      cloud 'openshift'
      // Label must be unique across the OpenShift namespace; slave pods
      // will be named after it
      label 'jenkins-slave-wwp'
      // The pod template we inheritFrom is auto-generated by the
      // OpenShift sync plug-in from a Kubernetes BuildConfig object
      // (see "Jenkins infrastructure ImageStreams and builds" in
      // ../tasks/continuous-integration.yml)
      inheritFrom "{{ ci_jenkins_slave_image_name }}"
      // Ideally we would want to not have a nodeSelector at all, but
      // we can't
      // (https://github.com/jenkinsci/kubernetes-plugin/pull/790).
      // The next best thing is to stipulate a label that all nodes
      // have (here, for an OpenShift 3.11 cluster running on Linux):
      nodeSelector "beta.kubernetes.io/os=linux"
      // Using the "yaml" field, we can weave whichever "sidekick"
      // containers we like into the pod template. The main "jnlp"
      // container (not pictured in the YAML) comes from the parent
      // (inheritFrom) template.
      yaml """
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: {{ ci_jenkins_test_sidekick_image_name }}
    image: {{ ci_jenkins_test_sidekick_image_name |
              docker_registry_path_qualified(namespace=openshift_namespace) }}
    command:
    - sleep
    - infinity

{#   # Besides the normal Kubernetes semantics, workingDir is also
     # where the Kubernetes Jenkins plug-in mounts the
     # "workspace-volume", which must be at the same path on all
     # Jenkins masters and slaves (so that paths served over JNLP make
     # sense). While the OpenShift sync plug-in takes care of that for
     # the "jnlp" container, we have to explicitly set it on sidekicks
     # or the `sh` command won't work inside a `container () { }`:
     #}
    workingDir: /tmp
"""
    }  // kubernetes
  }  // agent

  // Here we describe what the pipeline consists of.
  stages {

    stage('Assemblage de l\'image') {
      steps {
        // Imperative things (including any and all calls to
        // openshift.withCluster and friends) can only happen in a
        // "script".
        script {
          openshift.withCluster() {
            openshift.withProject() {
              def wpbase = openshift.selector('bc/wp-base')
              banner "Building bc/wp-base in namespace ${openshift.project()}:"
              wpbase.describe()

              def wpBaseBuild = wpbase.startBuild()
              watchBuilds(wpBaseBuild)
              def otherBuilds = getTriggeredBuilds(wpBaseBuild)
              watchBuilds(otherBuilds)
            }  // openshift.withProject
          }  // openshift.withCluster
        }  // script
      }  // steps
    }  // stage('Assemblage de l\'image')

    stage('DÃ©ploiement') {
      steps {
        script {
          openshift.withCluster() {
            openshift.withProject() {
              def latestDeploymentVersion = openshift.selector('dc', 'httpd-int').object().status.latestVersion
              def rc = openshift.selector('rc', "httpd-int-${latestDeploymentVersion}")
              Set mentioned = []
              rc.untilEach(1) {  // Wait until there is (at least) one
                                 // result in the selector, *and then*
                                 // each shall return true below:
                def rcMap = it.object()
                def name = rcMap.metadata.name
                def expectedReplicas = rcMap.status.replicas
                def actualReplicas = rcMap.status.readyReplicas
                if (! mentioned.contains(name)) {
                  echo "Waiting for rc/${name} to reach ${expectedReplicas} replicas (currently ${actualReplicas})"
                  mentioned.add(name)
                }
                return (expectedReplicas.equals(actualReplicas))
              }  // rc.untilEach
            }  // openshift.withProject
          }  // openshift.withCluster
        }  // script
      }  // steps
    }

    stage('Tests') {
      steps {
        container('{{ ci_jenkins_test_sidekick_image_name }}') {
          sh "/opt/app/bin/docker-entrypoint --test-target {{ ci_jenkins_test_target }} --screenshot-always || true"
        }
      }
    }  // stage('Tests')

  }  // stages

  // Here we set up some post-pipeline hooks to present the results.
  post {
    always {
      cucumber fileIncludePattern: '**/cucumber-report.json',
               sortingMethod: 'ALPHABETICAL',
               pendingStepsNumber: -1, skippedStepsNumber: -1,
               // We don't account scenarios and features - Only
               // individual steps can cause a test failure:
               failedFeaturesNumber: -1, failedScenariosNumber: -1
    }  // always
  }  // post
}  // pipeline


void watchBuilds(buildSelector) {
  // Coerce to selector
  if (buildSelector instanceof List<?>) {
    buildSelector = openshift.selector(buildSelector.collect {
      (it instanceof Map<?,?>) ? "build/${it.metadata.name}" : it
    })
  }

  def buildUrls = []
  buildSelector.withEach {
    buildUrls.push "{{ ci_jenkins_openshift_console_base_url }}/project/${openshift.project()}/browse/builds/${it.object().metadata.annotations['openshift.io/build-config.name']}/${it.object().metadata.name}?tab=logs"
  }
  if (buildUrls.size() > 0) {
    def hint = (
      ["Vous pouvez suivre la compilation plus confortablement depuis OpenShift :"]
      + buildUrls
    ).join("\n")
    banner hint
  }

  buildSelector.logs('-f')  // Waits for the build(s) to terminate
                            // (or mostly terminate - See below)
  timeout(time: 1, units: 'MINUTES') {
    // Despite logs having run their course, experience shows that
    // some builds can still be in "Running" state Kubernetes-wise:
    buildSelector.withEach {
      it.untilEach {
        return it.object().status.phase != "Running"
      }
    }
  }

  def failed = 0
  buildSelector.withEach {
    def status = it.object().status.phase
    if (status != 'Complete') {
      echo "Bad status for ${it.name()}: expected <Complete>, got <${status}>"
      failed++
    }
  }
  if (failed > 0) {
    banner "${failed} failed builds"
    sh "exit 1"  // https://stackoverflow.com/a/51642273/435004
  }
}

void banner (msg) {
  def length = msg.length() + 2
  if (length < 40) {
    length = 40
  }
  if (length > 80) {
    length = 80
  }
  def equals = "=" * length
  msg = [equals, msg, equals].join("\n")
  echo msg
}

void getTriggeredBuilds(buildSelector) {
  Set imageIDs = []
  buildSelector.withEach {
    def status = it.object().status
    if (status && status.output && status.output.to
        && status.output.to.imageDigest) {
      imageIDs.add(status.output.to.imageDigest)
    }
  }

  def triggeredBuilds = []
  for (build in openshift.selector("build").objects()) {
    if (build.spec && build.spec.triggeredBy) {
      for (triggered in build.spec.triggeredBy) {
        if (triggered.imageChangeBuild &&
            triggered.imageChangeBuild.imageID &&
            imageIDs.contains(
              triggered.imageChangeBuild.imageID.replaceFirst('.*@', '')))
        {
          triggeredBuilds.add(build)
        }
      }  // for
    }  // if
  }  // for

  def size = triggeredBuilds.size()
  return triggeredBuilds
}
