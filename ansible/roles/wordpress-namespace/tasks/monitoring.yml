- include_vars: monitoring-secrets.yml
  tags: always

- name: Secret - Bot Token (Telegram)
  kubernetes.core.k8s:
    definition:
      apiVersion: v1
      kind: Secret
      type: Opaque
      metadata:
        name: telegram-bot-token
        namespace: "{{ inventory_namespace }}"
      data:
        access_token: "{{ telegram_bot_token | b64encode }}"

- name: Monitoring - AlertmanagerConfig
  kubernetes.core.k8s:
    definition:
      apiVersion: monitoring.coreos.com/v1beta1
      kind: AlertmanagerConfig
      metadata:
        name: alertmanager-telegram
        namespace: "{{ inventory_namespace }}"
      spec:
        route:
          receiver: "telegram"
          groupBy: ["instance"]
          groupWait: 20s
          groupInterval: 5m
          repeatInterval: 3h
          matchers:
            - name: sendto
              value: telegram
              matchType: "="
        receivers:
          - name: "telegram"
            telegramConfigs:
              - botToken:
                  name: telegram-bot-token
                  key: access_token
                chatID: "{{ telegram_chat_id }}"
                message: |
                  {% raw %}
                  {{ range .Alerts }}
                  {{ if eq .Status "firing" }}
                  ðŸ”¥ FIRING -- {{ .Annotations.summary }}
                  {{ .Annotations.description }}
                  Started at (UTC): {{ .StartsAt.Format "2006-01-02 15:04:05" }}
                  {{ else if eq .Status "resolved" }}
                  âœ… RESOLVED -- {{ .Annotations.summary }}
                  {{ .Annotations.description }}
                  Started at (UTC): {{ .StartsAt.Format "2006-01-02 15:04:05" }}
                  Ended at (UTC): {{ .EndsAt.Format "2006-01-02 15:04:05" }}
                  {{ end }}
                  {{ end }}
                  {% endraw %}
                sendResolved: true

- name: Monitoring - PrometheusRule
  kubernetes.core.k8s:
    definition:
      apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      metadata:
        name: wordpress-alerts
        namespace: "{{ inventory_namespace }}"
      spec:
        groups:
          - name: menu-api-alerts
            rules:
              - alert: MenuApiTooManyRestarts
                expr: >
                  round(increase(kube_pod_container_status_restarts_total{namespace="{{ inventory_namespace }}", container="menu-api"}[{{ _menuapi_restarts_window_minutes }}m])) > 1
                for: "{{ _menuapi_restart_duration_minutes }}m"
                labels:
                  severity: critical
                  sendto: telegram
                annotations:
                  summary: "Too many Menu-Api restarts"
                  description: >-
                    Menu-API has restarted more than once in the last {{ _menuapi_restarts_window_minutes }} minutes.
              - alert: MenuApiPodCountMismatch
                expr: >
                  kube_deployment_status_replicas_available{deployment="menu-api", namespace="{{ inventory_namespace }}"} 
                  != kube_deployment_spec_replicas{deployment="menu-api", namespace="{{ inventory_namespace }}"}
                for: "{{ _menuapi_count_pods_duration_minutes }}m"
                labels:
                  severity: critical
                  sendto: telegram
                annotations:
                  summary: "Pod count mismatch (menu-api)"
                  description: >-
                    The deployment menu-api has not the desired number of pods for over {{ _menuapi_count_pods_duration_minutes }} minutes.
              - alert: MenuApiSlowRequests
                expr: >
                  histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job="menu-api", namespace="{{ inventory_namespace }}", route!="/favicon.ico", route!="/refresh"}[{{ _menuapi_slow_requests_window_minutes }}m])) by (le, route)) > {{ _menuapi_95_pct_threshold }}
                for: "{{ _menuapi_slow_requests_duration_minutes }}m"
                labels:
                  severity: critical
                  sendto: telegram
                annotations:
                  summary: "MenuAPI slow requests"
                  description: >-
                    menu-api is returning slow requests at 95% on the route
                    {% raw -%}
                     {{ $labels.route }} 
                    {%- endraw %}
                    with a rate greater than {{ _menuapi_95_pct_threshold }} over the last five minutes.
                    The condition has persist for at least {{ _menuapi_slow_requests_duration_minutes }} minutes.
          - name: php-fpm-alerts
            rules:
              - alert: PhpFpmHighProcessRequestDuration
                expr: >
                  phpfpm_process_request_duration > 30 * 1000 * 1000
                for: 2m
                labels:
                  severity: critical
                  sendto: telegram
                annotations:
                  summary: "PHP-FPM High process request duration"
                  description: >-
                    {% raw -%}
                    PHP-FPM process request duration exceeded 30s for more than 2 minutes.
                    (pod: {{ $labels.pod }}, instance: {{ $labels.instance }})
                    {%- endraw %}
          - name: nginx-alerts
            rules:
              - alert: NginxClientClosedRequests
                expr: >
                  rate(nginx_http_requests_total{status="499", namespace="{{ inventory_namespace }}"}[{{ _499s_rate_window_minutes }}m]) > {{ _499s_threshold }}
                for: "{{ _499s_duration_minutes }}m"
                labels:
                  severity: warning
                  sendto: telegram
                annotations:
                  summary: "nginx 499 Errors Detected"
                  description: >-
                    nginx is returning too many HTTP 499 errors (Client Closed Request) with a rate greater than {{ _499s_threshold }} over the last two minutes.
                    The condition has persist for at least {{ _499s_duration_minutes }} minutes.
                    {% raw -%}
                    (pod: {{ $labels.pod }}, instance: {{ $labels.instance }})
                    {%- endraw %}
              - alert: NginxInternalServerError
                expr: >
                  rate(nginx_http_requests_total{status="500", namespace="{{ inventory_namespace }}"}[{{ _500s_rate_window_minutes }}m]) > {{ _500s_threshold }}
                for: "{{ _500s_duration_minutes }}m"
                labels:
                  severity: critical
                  sendto: telegram
                annotations:
                  summary: "nginx 500 Errors Detected"
                  description: >-
                    nginx is returning too many HTTP 500 errors with a rate greater than {{ _500s_threshold }} over the last two minutes.
                    The condition has persist for at least {{ _500s_duration_minutes }} minutes.
                    {% raw -%}
                    (pod: {{ $labels.pod }}, instance: {{ $labels.instance }})
                    {%- endraw %}
              - alert: NginxGatewayTimeout
                expr: >
                  rate(nginx_http_requests_total{status="504", namespace="{{ inventory_namespace }}"}[{{ _504s_rate_window_minutes }}m]) > {{ _504s_threshold }}
                for: "{{ _504s_duration_minutes }}m"
                labels:
                  severity: critical
                  sendto: telegram
                annotations:
                  summary: "nginx 504 Errors Detected"
                  description: >-
                    nginx is returning too many HTTP 504 errors (Gateway Timeout) with a rate greater than {{ _504s_threshold }} over the last two minutes.
                    The condition has persist for at least {{ _504s_duration_minutes }} minutes.
                    {% raw -%}
                    (pod: {{ $labels.pod }}, instance: {{ $labels.instance }})
                    {%- endraw %}
          - name: mariadb-alerts
            rules:
              - alert: MariaDbTooManyRestarts
                # Matching by `pod=~` is required to weed out `migrate-*` pods
                expr: >
                  sum(round(increase(kube_pod_container_status_restarts_total{namespace="{{ inventory_namespace }}", container="mariadb", pod=~"mariadb-.*"}[{{ _mariadb_restarts_window_minutes }}m]))) > 1
                for: 3m
                labels:
                  severity: critical
                  sendto: telegram
                annotations:
                  summary: "Too many MariaDB restarts"
                  description: >-
                    {% raw -%}
                    More than once MariaDB (pod: {{ $labels.pod }}, instance: {{ $labels.instance }})
                    {%- endraw %}
                    has restarted in the last {{ _mariadb_restarts_window_minutes }} minutes.
          - name: pod-alerts
            rules:
              - alert: WordPressNginxPodCountMismatch
                expr: >
                  kube_deployment_status_replicas_available{deployment="wp-nginx", namespace="{{ inventory_namespace }}"} 
                  != kube_deployment_spec_replicas{deployment="wp-nginx", namespace="{{ inventory_namespace }}"}
                for: 3m
                labels:
                  severity: critical
                  sendto: telegram
                annotations:
                  summary: "Pod count mismatch (wp-nginx)"
                  description: "The deployment wp-nginx has not the desired number of pods for over 3 minutes."
          - name: monitoring-alerts
            rules:
              - alert: NoMonitoring
                expr: >
                  up{namespace="{{ inventory_namespace }}"} == 0
                for: 3m
                labels:
                  severity: critical
                  sendto: telegram
                annotations:
                  summary: "Monitoring target is down"
                  description: >-
                    {% raw -%}
                    The Prometheus target {{ $labels.endpoint }} (job: {{ $labels.job }}, pod: {{ $labels.pod }}, instance: {{ $labels.instance }})
                    has been down for over 3 minutes.
                    {%- endraw %}
  vars:
    _499s_threshold: 1
    _499s_rate_window_minutes: 2
    _499s_duration_minutes: 2
    _500s_threshold: 1
    _500s_rate_window_minutes: 2
    _500s_duration_minutes: 2
    _504s_threshold: 1
    _504s_rate_window_minutes: 2
    _504s_duration_minutes: 2
    _mariadb_restarts_window_minutes: 15
    _menuapi_restarts_window_minutes: 15
    _menuapi_restart_duration_minutes: 3
    _menuapi_slow_requests_window_minutes: 5
    _menuapi_95_pct_threshold: 0.02
    _menuapi_slow_requests_duration_minutes: 3
    _menuapi_count_pods_duration_minutes: 3
